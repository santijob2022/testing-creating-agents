{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-Blog-Generator\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langsmith import traceable\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "llm=ChatGroq(model=\"qwen-2.5-32b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "\n",
    "@traceable\n",
    "def arxiv_search(query):\n",
    "    \"\"\"\n",
    "        Search for the top 3 results according to the user query using ArxivAPIWrapper.\n",
    "        The information returned is a list with the top 3 related articles with at least the following information:\n",
    "        Title\n",
    "        Published\n",
    "        Authors\n",
    "        Summary\n",
    "        PDF_url\n",
    "    \"\"\"\n",
    "\n",
    "    # Using qwen-2.5-32b limits to 6000 per minute so I set doc_content_chars_max = 5900\n",
    "    arxiv = ArxivAPIWrapper(\n",
    "        top_k_results = 3,\n",
    "        ARXIV_MAX_QUERY_LENGTH = 300,\n",
    "        load_all_available_meta = True,\n",
    "        doc_content_chars_max = 5500\n",
    "    )\n",
    "\n",
    "    # Run Query and Get Results\n",
    "    # query = \"GAN's\"\n",
    "    results = arxiv.load(query)\n",
    "\n",
    "    for article in results:\n",
    "        print(f\"Title: {article.metadata.get('Title', 'N/A')}\")\n",
    "        print(f\"Published: {article.metadata.get('Published', 'N/A')}\")\n",
    "        print(f\"Authors: {article.metadata.get('Authors', 'N/A')}\\n\")\n",
    "        print(f\"Summary: {article.metadata.get('Summary', 'N/A')[:250]}\\n\")\n",
    "        print(f\"PDF url: {article.metadata.get('entry_id', 'N/A')}\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "\n",
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv_search]\n",
    "\n",
    "llm_with_tools=llm.bind_tools(tools,parallel_tool_calls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message\n",
    "sys_msg = SystemMessage(content=\n",
    "                        \"\"\"You are a helpful assistant tasked with creating a blog.\n",
    "                        Your first commitment is:\n",
    "                        1. Use the arxiv tool to search for the top 3 articles related to the user's question.\n",
    "\n",
    "                        2. Give the user the option to use wikipedia in case they want to research a term in the summaries.\n",
    "\n",
    "                        3. Wait for user's feedback\n",
    "\n",
    "                        4. If rejected try again from point 1.\n",
    "\n",
    "                        5. If accepted create a blog with the articles information.\n",
    "                     \n",
    "                        \"\"\")                      \n",
    "\n",
    "\n",
    "def assistant(state:MessagesState):\n",
    "    return {\"messages\":[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First I define a node that will be helpful for the human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import START, StateGraph, END\n",
    "# def human_feedback(state: MessagesState):\n",
    "#     \"\"\" Return the next node to execute \"\"\"\n",
    "\n",
    "#     # Check if human feedback\n",
    "#     # human_analyst_feedback=state.get('human_analyst_feedback', None)\n",
    "#     # if human_analyst_feedback:\n",
    "#     return \"tools\"\n",
    "    \n",
    "#     # Otherwise end\n",
    "#     return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "\n",
    "@traceable\n",
    "def human_feedback(state: MessagesState):\n",
    "    \"\"\" Return the next node to execute \"\"\"\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    if \"yes\" in user_input.lower():\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "## Define the node\n",
    "builder.add_node(\"assistant\",assistant)\n",
    "builder.add_node(\"human_feedback\",human_feedback)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "# Define edges\n",
    "builder.add_edge(START,\"assistant\")\n",
    "builder.add_edge(\"assistant\",\"tools\")\n",
    "builder.add_edge(\"tools\",\"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",tools_condition,[\"tools\",END]\n",
    ")\n",
    "# builder.add_edge( \"tools\",\"human_feedback\")\n",
    "\n",
    "#workflow.add_conditional_edges(\"generate_joke\",check_punchline,{\"Fail\":\"improve_joke\",\"Pass\":END})\n",
    "\n",
    "# Set up memory\n",
    "memory = MemorySaver()\n",
    "# Compile the graph with memory\n",
    "scienceBlogCreator = builder.compile(interrupt_before=[\"human_feedback\"],checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(scienceBlogCreator.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Agent\n",
    "\n",
    "#### First call to use the Arxiv tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoking the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "GANs\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_bk6x)\n",
      " Call ID: call_bk6x\n",
      "  Args:\n",
      "    query: GANs\n",
      "Title: Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey\n",
      "Published: 2021-11-26\n",
      "Authors: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley\n",
      "\n",
      "Summary: This is a tutorial and survey paper on Generative Adversarial Network (GAN),\n",
      "adversarial autoencoders, and their variants. We start with explaining\n",
      "adversarial learning and the vanilla GAN. Then, we explain the conditional GAN\n",
      "and DCGAN. The mode col\n",
      "\n",
      "PDF url: http://arxiv.org/abs/2111.13282v1\n",
      "\n",
      "Title: GAN You Do the GAN GAN?\n",
      "Published: 2019-04-01\n",
      "Authors: Joseph Suarez\n",
      "\n",
      "Summary: Generative Adversarial Networks (GANs) have become a dominant class of\n",
      "generative models. In recent years, GAN variants have yielded especially\n",
      "impressive results in the synthesis of a variety of forms of data. Examples\n",
      "include compelling natural and\n",
      "\n",
      "PDF url: http://arxiv.org/abs/1904.00724v1\n",
      "\n",
      "Title: Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances\n",
      "Published: 2023-03-27\n",
      "Authors: Arkanath Pathak, Nicholas Dufour\n",
      "\n",
      "Summary: Modern Generative Adversarial Networks (GANs) generate realistic images\n",
      "remarkably well. Previous work has demonstrated the feasibility of\n",
      "\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\n",
      "operate on images generated from a f\n",
      "\n",
      "PDF url: http://arxiv.org/abs/2303.15533v1\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "[Document(metadata={'Published': '2021-11-26', 'Title': 'Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey', 'Authors': 'Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley', 'Summary': 'This is a tutorial and survey paper on Generative Adversarial Network (GAN),\\nadversarial autoencoders, and their variants. We start with explaining\\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\\nand DCGAN. The mode collapse problem is introduced and various methods,\\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\\nintroduce some applications of GAN such as image-to-image translation\\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\\nGAN), text-to-image translation (including StackGAN), and mixing image\\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\\nautoencoders based on adversarial learning including adversarial autoencoder,\\nPixelGAN, and implicit autoencoder.', 'entry_id': 'http://arxiv.org/abs/2111.13282v1', 'published_first_time': '2021-11-26', 'comment': 'To appear as a part of an upcoming textbook on dimensionality\\n  reduction and manifold learning', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML'], 'links': ['http://arxiv.org/abs/2111.13282v1', 'http://arxiv.org/pdf/2111.13282v1']}, page_content='To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.\\nGenerative Adversarial Networks and Adversarial Autoencoders:\\nTutorial and Survey\\nBenyamin Ghojogh\\nBGHOJOGH@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAli Ghodsi\\nALI.GHODSI@UWATERLOO.CA\\nDepartment of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,\\nData Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada\\nFakhri Karray\\nKARRAY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nCentre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada\\nMark Crowley\\nMCROWLEY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAbstract\\nThis is a tutorial and survey paper on Generative\\nAdversarial Network (GAN), adversarial autoen-\\ncoders, and their variants. We start with explain-\\ning adversarial learning and the vanilla GAN.\\nThen, we explain the conditional GAN and\\nDCGAN. The mode collapse problem is intro-\\nduced and various methods, including minibatch\\nGAN, unrolled GAN, BourGAN, mixture GAN,\\nD2GAN, and Wasserstein GAN, are introduced\\nfor resolving this problem. Then, maximum like-\\nlihood estimation in GAN are explained along\\nwith f-GAN, adversarial variational Bayes, and\\nBayesian GAN. Then, we cover feature match-\\ning in GAN, InfoGAN, GRAN, LSGAN, energy-\\nbased GAN, CatGAN, MMD GAN, LapGAN,\\nprogressive GAN, triple GAN, LAG, GMAN,\\nAdaGAN, CoGAN, inverse GAN, BiGAN, ALI,\\nSAGAN, Few-shot GAN, SinGAN, and interpo-\\nlation and evaluation of GAN. Then, we intro-\\nduce some applications of GAN such as image-\\nto-image translation (including PatchGAN, Cy-\\ncleGAN, DeepFaceDrawing, simulated GAN, in-\\nteractive GAN), text-to-image translation (in-\\ncluding StackGAN), and mixing image charac-\\nteristics (including FineGAN and MixNMatch).\\nFinally, we explain the autoencoders based on\\nadversarial learning including adversarial au-\\ntoencoder, PixelGAN, and implicit autoencoder.\\n1. Introduction\\nSuppose we have a generative model which takes a ran-\\ndom noise as input and generates a data point. We want\\nthe generated data point to be of good quality; hence, we\\nshould somehow judge its quality. One way to judge it is\\nto observe the generated sample and assess its quality vi-\\nsually. In this case, the judge is a human. However, we\\ncannot take derivative of human’s judgment for optimiza-\\ntion. Generative Adversarial Network (GAN), proposed in\\n(Goodfellow et al., 2014), has the same idea but it can take\\nderivative of the judgment. For that, it uses a classiﬁer as\\nthe judge rather than a human. Hence, we have a generator\\ngenerating a sample and a binary classiﬁer (or discrimina-\\ntor) to classify the generated sample as a real or generated\\nsample. This classiﬁer can be a pre-trained network which\\nis already trained by some real and generated (fake) data\\npoints. However, GAN puts a step ahead and lets the clas-\\nsiﬁer be trained simultaneously with training the genera-\\ntor. This is the core idea of adversarial learning with the\\nclassiﬁer, also called the discriminator, and the generator\\ncompete each other; hence, they make each other stronger\\ngradually by this competition (Goodfellow et al., 2020).\\nIt is noteworthy that the term “adversarial” is used in two\\nmain streams of research in machine learning and they\\narXiv:2111.13282v1  [cs.LG]  26 Nov 2021\\nGenerative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey\\n2\\nshould not be confused. These two research areas are:\\n• Adversarial attack, also called learning with adversar-\\nial examples or adversarial machine learning. This\\nline of research inspects some examples which can\\nbe changed slightly but wisely to fool a trained learn-\\ning model. For example, perturbation of some speciﬁc\\npixels in the input image may change the decision of\\nlearning model. The reason for this can be analyzed\\ntheoretically. Some example works in this area are\\n(Huang et al., 2011; Moosavi-Dezfooli et al., 2016;\\nKurakin et al., 2017a;b; Madry et al., 2018).\\n• Adversarial learning for generation. This line of re-\\nsearch is categorized as generative models (Ng & Jor-\\ndan, 2002) and/or methods based on that. GAN is in\\nthis line of research. This paper focuses on this re-\\nsearch area.\\nAnother good tutorial on GAN is (Goodfellow, 2016) but it\\ndoes not cover most recent methods in adversarial learning.\\nAlso, an honorary introduction of GAN, by several main\\ncontributors of GAN, is (Goodfellow et al., 2020). Some\\nother existing surveys on GAN are (Wang et al., 2017;\\nCreswell et al., 2018; Gonog & Zhou, 2019; Hong et al.,\\n2019; Pan et al., 2019). This paper is a tutorial and survey\\non GAN and its variants.\\nRequired Background for the Reader\\nThis paper assumes that the reader has general knowledge\\nof calculus, probability, linear algebra, and basics of opti-\\nmization.\\n2. Generative Adversarial Network (GAN)\\n2.1. Adversarial Learning: The Adversarial Game\\nThe original GAN, also called the vanilla GAN, was\\nproposed in (Goodfellow et al., 2014).\\nConsider a d-\\ndimensional dataset with n data points, i.e., {xi\\n∈\\nRd}n\\ni=1. In GAN, we have a generator G which takes a\\np-dimensional random noise z ∈Rp as input and outputs\\na d-dimensional generated point x ∈Rd. Hence, it is the\\nmapping G : z →x where:\\nG(z) = x.\\n(1)\\nThe random noise can be seen as a latent f'), Document(metadata={'Published': '2019-04-01', 'Title': 'GAN You Do the GAN GAN?', 'Authors': 'Joseph Suarez', 'Summary': \"Generative Adversarial Networks (GANs) have become a dominant class of\\ngenerative models. In recent years, GAN variants have yielded especially\\nimpressive results in the synthesis of a variety of forms of data. Examples\\ninclude compelling natural and artistic images, textures, musical sequences,\\nand 3D object files. However, one obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning's most pressing questions: GAN you do\\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\\nGANs? We release the full source code for this project under the MIT license.\", 'entry_id': 'http://arxiv.org/abs/1904.00724v1', 'published_first_time': '2019-04-01', 'comment': '3 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.LG'], 'links': ['http://arxiv.org/abs/1904.00724v1', 'http://arxiv.org/pdf/1904.00724v1']}, page_content='GAN You Do the GAN GAN?\\nJoseph Suarez\\nAbstract\\nGenerative Adversarial Networks (GANs) have\\nbecome a dominant class of generative models.\\nIn recent years, GAN variants have yielded es-\\npecially impressive results in the synthesis of a\\nvariety of forms of data. Examples include com-\\npelling natural and artistic images, textures, mu-\\nsical sequences, and 3D object ﬁles. However,\\none obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning’s most\\npressing questions: GAN you do the GAN GAN?\\nThat is, is it possible to train a GAN to model a\\ndistribution of GANs? We release the full source\\ncode for this project under the MIT license.1\\n1. Introduction, Background, Related Work\\nGANs (Goodfellow et al., 2014) have become perhaps the\\nsingle most prevalent class of generative models in recent\\nyears, spawning hundreds of variants and an entire subﬁeld\\nof surrounding work. While they are notoriously difﬁcult to\\ntrain and often suffer from mode collapse, under the right\\nconditions, GANs have been shown to generate compelling\\nartiﬁcial data distributions and are perhaps best known for\\nrealistic image synthesis (Zhu et al., 2017).\\nOur objective is to apply GANs to a different class of data:\\nGANs themselves. Instead of using a GAN to model images,\\nwe use a GAN to model a distribution of GANs that model\\nimages. We refer to this architecture over architectures\\nas a GAN-GAN. We do not consider GAN-GAN-GANs\\nor GAN-GAN-GAN-GANs in this work. While these are\\nstraightforward to implement, they would require an ex-\\nponential parameter budget (at least as formulated in the\\npresent work) which we lack the hardware to support.\\nHypernetworks (Ha et al., 2016) and extensions thereof\\n(Deutsch, 2018; Suarez, 2017) have also explored the con-\\ncept of using one network to generate the weights of another.\\nHowever, to our knowledge, all such settings typically learn\\nboth the architecture and the meta-architecture end-to-end.\\nIn contrast, we are interested in whether it is possible to\\ndirectly learn a distribution over architectures within the\\n1Full source code:\\nhttps://github.com/jsuarez5341/gan-you-do-the-gan-gan\\nFigure 1. GAN-GAN: a GAN trained on a dataset of GANs\\nstandard setting of generative modeling. We treat GANs\\nthemselves as examples and learn a GAN-GAN by training\\non small set of trained GANs.\\nThis paper is a joke; however, the results are in fact real.\\nInterpretability is a key problem in deep learning, especially\\nin models to be deployed in real world systems. Generative\\nmodeling over networks could serve as a useful tool for\\nvisualization and analysis of network decisions and results.\\n2. Methods\\nGANs formulate a two player game between networks. For-\\nmally, GANs deﬁnes a Generator G and a Discriminator\\nD. The Discriminator models the probability P(G|x) that\\na given example x is fake. The Generator maximizes the\\nprobability P(D(G(z))) (z is sampled noise) that the Dis-\\ncriminator will output an incorrect prediction. A GAN-GAN\\nis simply a GAN trained on a dataset of GAN weights.\\nAlgorithm 1 GAN-GAN Training. We ﬁrst train a set of\\nGANs and save snapshots of the parameters each epoch. We\\nthen train a GAN-GAN (a GAN over GANS) by treating\\neach snapshot as an individual training example.\\nfor GAN Index = 1...#Networks do\\nInitialize an MNIST GAN\\nfor Epoch = 1...#Epochs do\\nTrain the MNIST GAN for one epoch\\nSave a snapshot of the GAN parameters\\nend for\\nend for\\nLoad all snapshots of all GANs into a dataset with #Net-\\nworks × #Epochs examples\\nTrain a GAN over the dataset of GAN snapshots\\narXiv:1904.00724v1  [cs.CV]  1 Apr 2019\\nGAN-GAN\\nFigure 2. Example samples from the training of an MNIST GAN (top-bottom left-right: epochs 1, 2, 10, 25, 27, 30, 32, 35, 40, 49)\\nFigure 3. Image samples from GANs sampled from the trained GAN-GAN. Rows correspond to GANs linearly sampled from 1D\\nGAN-GAN latent space in the interval (-2, 2). Columns correspond to a particular noise vector input to all GANs.\\nGAN-GAN\\n3. Experiments and Discussion\\nGAN Architecture\\nThe generator and discriminator are\\nthree layer (input-hidden-output) fully connected neural net-\\nworks with hidden dimension 64. We use Leaky ReLU(Xu\\net al., 2015) activations with 0.2 negative slope after the\\ninput and hidden layers. We use tanh for the generator out-\\nput (dimensionality 28 × 28 = 784 to match MNIST) and\\nsigmoid for the discriminator output (dimensionality 1). The\\ngenerator samples from latent dimension 64.\\nGAN-GAN Architecture\\nThe GAN-GAN generator and\\ndiscriminator have the same layer and activation structure\\nas the MNIST GAN. The input dimensionality is 113745,\\nwhich is equal to the dimensionality of the GAN parameter\\nvector. We found that using a smaller hidden dimension\\nfor the discriminator (8) than the generator (64) helped to\\nstabilize training. We use latent dimension 1 for the GAN-\\nGAN in order to enable visualizations. Results improve\\nwith a larger latent space.\\nTraining\\nWe use Adam(Kingma & Ba, 2014) for all net-\\nworks. The learning rate is ﬁxed to 0.0002; all other pa-\\nrameters are PyTorch defaults. We use batch sizes 128 and\\n32 for MNIST and the GAN-GAN, respectively. As de-\\nscribed in Algorithm 1, we train 35 MNIST GANs for 100\\nepochs each, saving snapshots of the weights at each epoch.\\nWe train the GAN-GAN for 250 epochs using these 3500\\nsnapshots as training examples.\\nResults\\nIn order to evaluate the performance of the GAN-\\nGAN, we ﬁrst linearly sample 32 GANs from the 1-\\ndimensional latent space of the GAN-GAN. We then ﬁx\\n40 noise'), Document(metadata={'Published': '2023-03-27', 'Title': 'Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances', 'Authors': 'Arkanath Pathak, Nicholas Dufour', 'Summary': 'Modern Generative Adversarial Networks (GANs) generate realistic images\\nremarkably well. Previous work has demonstrated the feasibility of\\n\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\\noperate on images generated from a frozen GAN. That such classifiers work at\\nall affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts\\nacross samples) present in GAN training. We iteratively train GAN-classifiers\\nand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge\\ngaps), and examine the effect on GAN training dynamics, output quality, and\\nGAN-classifier generalization. We investigate two settings, a small DCGAN\\narchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA\\nGAN architecture trained on high dimensional images (FFHQ). We find that the\\nDCGAN is unable to effectively fool a held-out GAN-classifier without\\ncompromising the output quality. However, StyleGAN2 can fool held-out\\nclassifiers with no change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to reveal an ordering\\nover optima in the generator parameter space. Finally, we study different\\nclassifier architectures and show that the architecture of the GAN-classifier\\nhas a strong influence on the set of its learned artifacts.', 'entry_id': 'http://arxiv.org/abs/2303.15533v1', 'published_first_time': '2023-03-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV'], 'links': ['http://arxiv.org/abs/2303.15533v1', 'http://arxiv.org/pdf/2303.15533v1']}, page_content='Sequential training of GANs against GAN-classifiers reveals correlated\\n“knowledge gaps” present among independently trained GAN instances\\nArkanath Pathak\\nNicholas Dufour\\nGoogle Research\\n{arkanath,ndufour}@google.com\\nAbstract\\nModern Generative Adversarial Networks (GANs) gen-\\nerate realistic images remarkably well. Previous work has\\ndemonstrated the feasibility of “GAN-classifiers” that are\\ndistinct from the co-trained discriminator, and operate on\\nimages generated from a frozen GAN. That such classifiers\\nwork at all affirms the existence of “knowledge gaps” (out-of-\\ndistribution artifacts across samples) present in GAN train-\\ning. We iteratively train GAN-classifiers and train GANs that\\n“fool” the classifiers (in an attempt to fill the knowledge gaps),\\nand examine the effect on GAN training dynamics, output\\nquality, and GAN-classifier generalization. We investigate\\ntwo settings, a small DCGAN architecture trained on low\\ndimensional images (MNIST), and StyleGAN2, a SOTA GAN\\narchitecture trained on high dimensional images (FFHQ).\\nWe find that the DCGAN is unable to effectively fool a held-\\nout GAN-classifier without compromising the output quality.\\nHowever, the StyleGAN2 can fool held-out classifiers with\\nno change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to\\nreveal an ordering over optima in the generator parameter\\nspace. Finally, we study different classifier architectures and\\nshow that the architecture of the GAN-classifier has a strong\\ninfluence on the set of its learned artifacts.\\n1. Introduction\\nGAN [9] architectures like StyleGAN2 [18] generate\\nhigh-resolution images that appear largely indistinguishable\\nfrom real images to the untrained eye [15, 19, 25]. While\\nthere are many positive applications, the ability to generate\\nlarge amounts of realistic images is also a source of concern\\ngiven its potential application in scaled abuse and misin-\\nformation. In particular, GAN-generated human faces are\\nwidely available (e.g., thispersondoesnotexist.com) and have\\nbeen used for creating fake identities on the internet [13].\\nDetection of GAN-generated images is an active research\\narea (see [10] for a survey of approaches), with some us-\\ning custom methods and others using generic CNN-based\\nclassifiers. Such classifiers are distinct from the discrimina-\\ntor networks that are trained alongside the generator in the\\narchetypal GAN setup. Given the adversarial nature of the\\ntraining loss for GANs, the existence of the GAN-classifiers\\nsuggest consistent generator knowledge gaps (i.e., artifacts\\npresent across samples that distinguish generated images\\nfrom those of the underlying distribution) left by discrimi-\\nnators during training. Specialized classifiers [32] are able\\nto detect images sampled from held-out GAN instances and\\neven from held-out GAN architectures. These generalization\\ncapabilities imply that the knowledge gaps are consistent\\nnot only across samples from a GAN generator but across\\nindependent GAN generator instances.\\nIn this work we modify the GAN training loss in order\\nto fool a GAN-classifier in addition to the co-trained dis-\\ncriminator, and examine the effect on training dynamics and\\noutput quality. We conduct multiple rounds of training in-\\ndependent pools (initialized differently) of GANs followed\\nby GAN-classifiers, and gain new insights into the GAN\\noptimization process. We investigate two different settings:\\nin the first setting, we choose the low-dimensional domain\\nof handwritten digits (MNIST [20]), using a small DCGAN\\n[26] architecture and a vanilla GAN-classifier architecture.\\nFor the second setting, we choose a high-dimensional do-\\nmain of human faces (FFHQ [17]) with StyleGAN2 (SG2)\\nas a SOTA GAN architecture, and three different GAN-\\nclassifier architectures (ResNet-50 [11], Inception-v3 [29],\\nand MobileNetV2 [28]). Our findings in this paper are as\\nfollows:\\n• Samples drawn from a GAN instance exhibit a space of\\n“artifacts” that are exploited by the classifiers, and this\\nspace is strongly correlated with those of other GAN\\ngenerator instances. This effect is present in both the\\nDCGAN and SG2 settings.\\n• Upon introducing the need to fool held-out classifiers,\\nthe DCGAN is unable to generate high quality outputs.\\n• In the high dimensional setting, however, SG2 gener-\\nators can easily fool held-out trained classifiers, and\\nmove to a new artifact space. Strikingly, we find that\\nthe artifact space is correlated among the new popula-\\ntion of generators as it was in the original population.\\nThis correlation appears to persist in subsequent rounds\\nas new classifiers are introduced that are adapted to the\\nnew artifact spaces.\\n• MobileNetV2 classifier instances in the SG2 setting\\nappear unable to learn all of the artifacts available for\\nthem to exploit. Instead, MobileNetV2 instances form\\nclusters based on the subset of artifacts learned. We\\nhypothesize this being an effect of classifier capacity.\\n• An SG2 generator trained to reliably fool unseen classi-\\nfier instances from a given architecture is not guaran-\\nteed to fool classifiers from another architecture. There-\\nfore, the artifacts learned by a given classifier depends\\nstrongly on the classifier’s architecture.\\n2. Related Work\\nResearch into detection of GAN-generated media has\\nlargely tracked the increasing prominence and output quality\\nof GANs themselves. Several studies [4, 5, 6, 10, 14, 22,\\n24, 31, 32] focus on detection of GAN-generated images\\nusing CNNs, and thei')]\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "# user_input=input(\"What do you want to research about?\")\n",
    "user_input=\"GANs\"\n",
    "messages = {\"messages\": HumanMessage(content=user_input)}\n",
    "\n",
    "thread={\"configurable\":{\"thread_id\":\"arxiv_call_1\"}}\n",
    "# messages = scienceBlogCreator.invoke({\"messages\":messages})\n",
    "# messages={'messages':HumanMessage(content=messages)}\n",
    "# scienceBlogCreator.stream(messages,thread,stream_mode=\"values\")\n",
    "for event in scienceBlogCreator.stream(messages,thread,stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "GANs\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_bk6x)\n",
      " Call ID: call_bk6x\n",
      "  Args:\n",
      "    query: GANs\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "[Document(metadata={'Published': '2021-11-26', 'Title': 'Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey', 'Authors': 'Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley', 'Summary': 'This is a tutorial and survey paper on Generative Adversarial Network (GAN),\\nadversarial autoencoders, and their variants. We start with explaining\\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\\nand DCGAN. The mode collapse problem is introduced and various methods,\\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\\nintroduce some applications of GAN such as image-to-image translation\\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\\nGAN), text-to-image translation (including StackGAN), and mixing image\\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\\nautoencoders based on adversarial learning including adversarial autoencoder,\\nPixelGAN, and implicit autoencoder.', 'entry_id': 'http://arxiv.org/abs/2111.13282v1', 'published_first_time': '2021-11-26', 'comment': 'To appear as a part of an upcoming textbook on dimensionality\\n  reduction and manifold learning', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML'], 'links': ['http://arxiv.org/abs/2111.13282v1', 'http://arxiv.org/pdf/2111.13282v1']}, page_content='To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.\\nGenerative Adversarial Networks and Adversarial Autoencoders:\\nTutorial and Survey\\nBenyamin Ghojogh\\nBGHOJOGH@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAli Ghodsi\\nALI.GHODSI@UWATERLOO.CA\\nDepartment of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,\\nData Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada\\nFakhri Karray\\nKARRAY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nCentre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada\\nMark Crowley\\nMCROWLEY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAbstract\\nThis is a tutorial and survey paper on Generative\\nAdversarial Network (GAN), adversarial autoen-\\ncoders, and their variants. We start with explain-\\ning adversarial learning and the vanilla GAN.\\nThen, we explain the conditional GAN and\\nDCGAN. The mode collapse problem is intro-\\nduced and various methods, including minibatch\\nGAN, unrolled GAN, BourGAN, mixture GAN,\\nD2GAN, and Wasserstein GAN, are introduced\\nfor resolving this problem. Then, maximum like-\\nlihood estimation in GAN are explained along\\nwith f-GAN, adversarial variational Bayes, and\\nBayesian GAN. Then, we cover feature match-\\ning in GAN, InfoGAN, GRAN, LSGAN, energy-\\nbased GAN, CatGAN, MMD GAN, LapGAN,\\nprogressive GAN, triple GAN, LAG, GMAN,\\nAdaGAN, CoGAN, inverse GAN, BiGAN, ALI,\\nSAGAN, Few-shot GAN, SinGAN, and interpo-\\nlation and evaluation of GAN. Then, we intro-\\nduce some applications of GAN such as image-\\nto-image translation (including PatchGAN, Cy-\\ncleGAN, DeepFaceDrawing, simulated GAN, in-\\nteractive GAN), text-to-image translation (in-\\ncluding StackGAN), and mixing image charac-\\nteristics (including FineGAN and MixNMatch).\\nFinally, we explain the autoencoders based on\\nadversarial learning including adversarial au-\\ntoencoder, PixelGAN, and implicit autoencoder.\\n1. Introduction\\nSuppose we have a generative model which takes a ran-\\ndom noise as input and generates a data point. We want\\nthe generated data point to be of good quality; hence, we\\nshould somehow judge its quality. One way to judge it is\\nto observe the generated sample and assess its quality vi-\\nsually. In this case, the judge is a human. However, we\\ncannot take derivative of human’s judgment for optimiza-\\ntion. Generative Adversarial Network (GAN), proposed in\\n(Goodfellow et al., 2014), has the same idea but it can take\\nderivative of the judgment. For that, it uses a classiﬁer as\\nthe judge rather than a human. Hence, we have a generator\\ngenerating a sample and a binary classiﬁer (or discrimina-\\ntor) to classify the generated sample as a real or generated\\nsample. This classiﬁer can be a pre-trained network which\\nis already trained by some real and generated (fake) data\\npoints. However, GAN puts a step ahead and lets the clas-\\nsiﬁer be trained simultaneously with training the genera-\\ntor. This is the core idea of adversarial learning with the\\nclassiﬁer, also called the discriminator, and the generator\\ncompete each other; hence, they make each other stronger\\ngradually by this competition (Goodfellow et al., 2020).\\nIt is noteworthy that the term “adversarial” is used in two\\nmain streams of research in machine learning and they\\narXiv:2111.13282v1  [cs.LG]  26 Nov 2021\\nGenerative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey\\n2\\nshould not be confused. These two research areas are:\\n• Adversarial attack, also called learning with adversar-\\nial examples or adversarial machine learning. This\\nline of research inspects some examples which can\\nbe changed slightly but wisely to fool a trained learn-\\ning model. For example, perturbation of some speciﬁc\\npixels in the input image may change the decision of\\nlearning model. The reason for this can be analyzed\\ntheoretically. Some example works in this area are\\n(Huang et al., 2011; Moosavi-Dezfooli et al., 2016;\\nKurakin et al., 2017a;b; Madry et al., 2018).\\n• Adversarial learning for generation. This line of re-\\nsearch is categorized as generative models (Ng & Jor-\\ndan, 2002) and/or methods based on that. GAN is in\\nthis line of research. This paper focuses on this re-\\nsearch area.\\nAnother good tutorial on GAN is (Goodfellow, 2016) but it\\ndoes not cover most recent methods in adversarial learning.\\nAlso, an honorary introduction of GAN, by several main\\ncontributors of GAN, is (Goodfellow et al., 2020). Some\\nother existing surveys on GAN are (Wang et al., 2017;\\nCreswell et al., 2018; Gonog & Zhou, 2019; Hong et al.,\\n2019; Pan et al., 2019). This paper is a tutorial and survey\\non GAN and its variants.\\nRequired Background for the Reader\\nThis paper assumes that the reader has general knowledge\\nof calculus, probability, linear algebra, and basics of opti-\\nmization.\\n2. Generative Adversarial Network (GAN)\\n2.1. Adversarial Learning: The Adversarial Game\\nThe original GAN, also called the vanilla GAN, was\\nproposed in (Goodfellow et al., 2014).\\nConsider a d-\\ndimensional dataset with n data points, i.e., {xi\\n∈\\nRd}n\\ni=1. In GAN, we have a generator G which takes a\\np-dimensional random noise z ∈Rp as input and outputs\\na d-dimensional generated point x ∈Rd. Hence, it is the\\nmapping G : z →x where:\\nG(z) = x.\\n(1)\\nThe random noise can be seen as a latent f'), Document(metadata={'Published': '2019-04-01', 'Title': 'GAN You Do the GAN GAN?', 'Authors': 'Joseph Suarez', 'Summary': \"Generative Adversarial Networks (GANs) have become a dominant class of\\ngenerative models. In recent years, GAN variants have yielded especially\\nimpressive results in the synthesis of a variety of forms of data. Examples\\ninclude compelling natural and artistic images, textures, musical sequences,\\nand 3D object files. However, one obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning's most pressing questions: GAN you do\\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\\nGANs? We release the full source code for this project under the MIT license.\", 'entry_id': 'http://arxiv.org/abs/1904.00724v1', 'published_first_time': '2019-04-01', 'comment': '3 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.LG'], 'links': ['http://arxiv.org/abs/1904.00724v1', 'http://arxiv.org/pdf/1904.00724v1']}, page_content='GAN You Do the GAN GAN?\\nJoseph Suarez\\nAbstract\\nGenerative Adversarial Networks (GANs) have\\nbecome a dominant class of generative models.\\nIn recent years, GAN variants have yielded es-\\npecially impressive results in the synthesis of a\\nvariety of forms of data. Examples include com-\\npelling natural and artistic images, textures, mu-\\nsical sequences, and 3D object ﬁles. However,\\none obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning’s most\\npressing questions: GAN you do the GAN GAN?\\nThat is, is it possible to train a GAN to model a\\ndistribution of GANs? We release the full source\\ncode for this project under the MIT license.1\\n1. Introduction, Background, Related Work\\nGANs (Goodfellow et al., 2014) have become perhaps the\\nsingle most prevalent class of generative models in recent\\nyears, spawning hundreds of variants and an entire subﬁeld\\nof surrounding work. While they are notoriously difﬁcult to\\ntrain and often suffer from mode collapse, under the right\\nconditions, GANs have been shown to generate compelling\\nartiﬁcial data distributions and are perhaps best known for\\nrealistic image synthesis (Zhu et al., 2017).\\nOur objective is to apply GANs to a different class of data:\\nGANs themselves. Instead of using a GAN to model images,\\nwe use a GAN to model a distribution of GANs that model\\nimages. We refer to this architecture over architectures\\nas a GAN-GAN. We do not consider GAN-GAN-GANs\\nor GAN-GAN-GAN-GANs in this work. While these are\\nstraightforward to implement, they would require an ex-\\nponential parameter budget (at least as formulated in the\\npresent work) which we lack the hardware to support.\\nHypernetworks (Ha et al., 2016) and extensions thereof\\n(Deutsch, 2018; Suarez, 2017) have also explored the con-\\ncept of using one network to generate the weights of another.\\nHowever, to our knowledge, all such settings typically learn\\nboth the architecture and the meta-architecture end-to-end.\\nIn contrast, we are interested in whether it is possible to\\ndirectly learn a distribution over architectures within the\\n1Full source code:\\nhttps://github.com/jsuarez5341/gan-you-do-the-gan-gan\\nFigure 1. GAN-GAN: a GAN trained on a dataset of GANs\\nstandard setting of generative modeling. We treat GANs\\nthemselves as examples and learn a GAN-GAN by training\\non small set of trained GANs.\\nThis paper is a joke; however, the results are in fact real.\\nInterpretability is a key problem in deep learning, especially\\nin models to be deployed in real world systems. Generative\\nmodeling over networks could serve as a useful tool for\\nvisualization and analysis of network decisions and results.\\n2. Methods\\nGANs formulate a two player game between networks. For-\\nmally, GANs deﬁnes a Generator G and a Discriminator\\nD. The Discriminator models the probability P(G|x) that\\na given example x is fake. The Generator maximizes the\\nprobability P(D(G(z))) (z is sampled noise) that the Dis-\\ncriminator will output an incorrect prediction. A GAN-GAN\\nis simply a GAN trained on a dataset of GAN weights.\\nAlgorithm 1 GAN-GAN Training. We ﬁrst train a set of\\nGANs and save snapshots of the parameters each epoch. We\\nthen train a GAN-GAN (a GAN over GANS) by treating\\neach snapshot as an individual training example.\\nfor GAN Index = 1...#Networks do\\nInitialize an MNIST GAN\\nfor Epoch = 1...#Epochs do\\nTrain the MNIST GAN for one epoch\\nSave a snapshot of the GAN parameters\\nend for\\nend for\\nLoad all snapshots of all GANs into a dataset with #Net-\\nworks × #Epochs examples\\nTrain a GAN over the dataset of GAN snapshots\\narXiv:1904.00724v1  [cs.CV]  1 Apr 2019\\nGAN-GAN\\nFigure 2. Example samples from the training of an MNIST GAN (top-bottom left-right: epochs 1, 2, 10, 25, 27, 30, 32, 35, 40, 49)\\nFigure 3. Image samples from GANs sampled from the trained GAN-GAN. Rows correspond to GANs linearly sampled from 1D\\nGAN-GAN latent space in the interval (-2, 2). Columns correspond to a particular noise vector input to all GANs.\\nGAN-GAN\\n3. Experiments and Discussion\\nGAN Architecture\\nThe generator and discriminator are\\nthree layer (input-hidden-output) fully connected neural net-\\nworks with hidden dimension 64. We use Leaky ReLU(Xu\\net al., 2015) activations with 0.2 negative slope after the\\ninput and hidden layers. We use tanh for the generator out-\\nput (dimensionality 28 × 28 = 784 to match MNIST) and\\nsigmoid for the discriminator output (dimensionality 1). The\\ngenerator samples from latent dimension 64.\\nGAN-GAN Architecture\\nThe GAN-GAN generator and\\ndiscriminator have the same layer and activation structure\\nas the MNIST GAN. The input dimensionality is 113745,\\nwhich is equal to the dimensionality of the GAN parameter\\nvector. We found that using a smaller hidden dimension\\nfor the discriminator (8) than the generator (64) helped to\\nstabilize training. We use latent dimension 1 for the GAN-\\nGAN in order to enable visualizations. Results improve\\nwith a larger latent space.\\nTraining\\nWe use Adam(Kingma & Ba, 2014) for all net-\\nworks. The learning rate is ﬁxed to 0.0002; all other pa-\\nrameters are PyTorch defaults. We use batch sizes 128 and\\n32 for MNIST and the GAN-GAN, respectively. As de-\\nscribed in Algorithm 1, we train 35 MNIST GANs for 100\\nepochs each, saving snapshots of the weights at each epoch.\\nWe train the GAN-GAN for 250 epochs using these 3500\\nsnapshots as training examples.\\nResults\\nIn order to evaluate the performance of the GAN-\\nGAN, we ﬁrst linearly sample 32 GANs from the 1-\\ndimensional latent space of the GAN-GAN. We then ﬁx\\n40 noise'), Document(metadata={'Published': '2023-03-27', 'Title': 'Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances', 'Authors': 'Arkanath Pathak, Nicholas Dufour', 'Summary': 'Modern Generative Adversarial Networks (GANs) generate realistic images\\nremarkably well. Previous work has demonstrated the feasibility of\\n\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\\noperate on images generated from a frozen GAN. That such classifiers work at\\nall affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts\\nacross samples) present in GAN training. We iteratively train GAN-classifiers\\nand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge\\ngaps), and examine the effect on GAN training dynamics, output quality, and\\nGAN-classifier generalization. We investigate two settings, a small DCGAN\\narchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA\\nGAN architecture trained on high dimensional images (FFHQ). We find that the\\nDCGAN is unable to effectively fool a held-out GAN-classifier without\\ncompromising the output quality. However, StyleGAN2 can fool held-out\\nclassifiers with no change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to reveal an ordering\\nover optima in the generator parameter space. Finally, we study different\\nclassifier architectures and show that the architecture of the GAN-classifier\\nhas a strong influence on the set of its learned artifacts.', 'entry_id': 'http://arxiv.org/abs/2303.15533v1', 'published_first_time': '2023-03-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV'], 'links': ['http://arxiv.org/abs/2303.15533v1', 'http://arxiv.org/pdf/2303.15533v1']}, page_content='Sequential training of GANs against GAN-classifiers reveals correlated\\n“knowledge gaps” present among independently trained GAN instances\\nArkanath Pathak\\nNicholas Dufour\\nGoogle Research\\n{arkanath,ndufour}@google.com\\nAbstract\\nModern Generative Adversarial Networks (GANs) gen-\\nerate realistic images remarkably well. Previous work has\\ndemonstrated the feasibility of “GAN-classifiers” that are\\ndistinct from the co-trained discriminator, and operate on\\nimages generated from a frozen GAN. That such classifiers\\nwork at all affirms the existence of “knowledge gaps” (out-of-\\ndistribution artifacts across samples) present in GAN train-\\ning. We iteratively train GAN-classifiers and train GANs that\\n“fool” the classifiers (in an attempt to fill the knowledge gaps),\\nand examine the effect on GAN training dynamics, output\\nquality, and GAN-classifier generalization. We investigate\\ntwo settings, a small DCGAN architecture trained on low\\ndimensional images (MNIST), and StyleGAN2, a SOTA GAN\\narchitecture trained on high dimensional images (FFHQ).\\nWe find that the DCGAN is unable to effectively fool a held-\\nout GAN-classifier without compromising the output quality.\\nHowever, the StyleGAN2 can fool held-out classifiers with\\nno change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to\\nreveal an ordering over optima in the generator parameter\\nspace. Finally, we study different classifier architectures and\\nshow that the architecture of the GAN-classifier has a strong\\ninfluence on the set of its learned artifacts.\\n1. Introduction\\nGAN [9] architectures like StyleGAN2 [18] generate\\nhigh-resolution images that appear largely indistinguishable\\nfrom real images to the untrained eye [15, 19, 25]. While\\nthere are many positive applications, the ability to generate\\nlarge amounts of realistic images is also a source of concern\\ngiven its potential application in scaled abuse and misin-\\nformation. In particular, GAN-generated human faces are\\nwidely available (e.g., thispersondoesnotexist.com) and have\\nbeen used for creating fake identities on the internet [13].\\nDetection of GAN-generated images is an active research\\narea (see [10] for a survey of approaches), with some us-\\ning custom methods and others using generic CNN-based\\nclassifiers. Such classifiers are distinct from the discrimina-\\ntor networks that are trained alongside the generator in the\\narchetypal GAN setup. Given the adversarial nature of the\\ntraining loss for GANs, the existence of the GAN-classifiers\\nsuggest consistent generator knowledge gaps (i.e., artifacts\\npresent across samples that distinguish generated images\\nfrom those of the underlying distribution) left by discrimi-\\nnators during training. Specialized classifiers [32] are able\\nto detect images sampled from held-out GAN instances and\\neven from held-out GAN architectures. These generalization\\ncapabilities imply that the knowledge gaps are consistent\\nnot only across samples from a GAN generator but across\\nindependent GAN generator instances.\\nIn this work we modify the GAN training loss in order\\nto fool a GAN-classifier in addition to the co-trained dis-\\ncriminator, and examine the effect on training dynamics and\\noutput quality. We conduct multiple rounds of training in-\\ndependent pools (initialized differently) of GANs followed\\nby GAN-classifiers, and gain new insights into the GAN\\noptimization process. We investigate two different settings:\\nin the first setting, we choose the low-dimensional domain\\nof handwritten digits (MNIST [20]), using a small DCGAN\\n[26] architecture and a vanilla GAN-classifier architecture.\\nFor the second setting, we choose a high-dimensional do-\\nmain of human faces (FFHQ [17]) with StyleGAN2 (SG2)\\nas a SOTA GAN architecture, and three different GAN-\\nclassifier architectures (ResNet-50 [11], Inception-v3 [29],\\nand MobileNetV2 [28]). Our findings in this paper are as\\nfollows:\\n• Samples drawn from a GAN instance exhibit a space of\\n“artifacts” that are exploited by the classifiers, and this\\nspace is strongly correlated with those of other GAN\\ngenerator instances. This effect is present in both the\\nDCGAN and SG2 settings.\\n• Upon introducing the need to fool held-out classifiers,\\nthe DCGAN is unable to generate high quality outputs.\\n• In the high dimensional setting, however, SG2 gener-\\nators can easily fool held-out trained classifiers, and\\nmove to a new artifact space. Strikingly, we find that\\nthe artifact space is correlated among the new popula-\\ntion of generators as it was in the original population.\\nThis correlation appears to persist in subsequent rounds\\nas new classifiers are introduced that are adapted to the\\nnew artifact spaces.\\n• MobileNetV2 classifier instances in the SG2 setting\\nappear unable to learn all of the artifacts available for\\nthem to exploit. Instead, MobileNetV2 instances form\\nclusters based on the subset of artifacts learned. We\\nhypothesize this being an effect of classifier capacity.\\n• An SG2 generator trained to reliably fool unseen classi-\\nfier instances from a given architecture is not guaran-\\nteed to fool classifiers from another architecture. There-\\nfore, the artifacts learned by a given classifier depends\\nstrongly on the classifier’s architecture.\\n2. Related Work\\nResearch into detection of GAN-generated media has\\nlargely tracked the increasing prominence and output quality\\nof GANs themselves. Several studies [4, 5, 6, 10, 14, 22,\\n24, 31, 32] focus on detection of GAN-generated images\\nusing CNNs, and thei')]\n"
     ]
    }
   ],
   "source": [
    "new_state = scienceBlogCreator.get_state(thread).values\n",
    "for m in new_state['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scienceBlogCreator.get_state(thread).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PregelTask(id='f898353c-9e4f-9c58-84fb-1534444a5cde', name='human_feedback', path=('__pregel_pull', 'human_feedback'), error=None, interrupts=(), state=None, result=None),)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scienceBlogCreator.get_state(thread).tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the state asking to the user if they want to research a topic in wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': 'arxiv_call_1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1efff63a-369b-676f-8003-38cb4bd52bb6'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_input=input(\"Do you want to clarify any term by using Wikipedia?\")\n",
    "user_input=\"yes, research what is a networks\"\n",
    "scienceBlogCreator.update_state(\n",
    "    thread,\n",
    "    {\"messages\":[\n",
    "        SystemMessage(content=\"\"\"Based on the next user answer, decide if you should return to the tools \n",
    "                      node to make a wikipedia search or if you should go to END.\"\"\"),\n",
    "        HumanMessage(content=user_input)\n",
    "        ]},\n",
    "    #  as_node=\"human_feedback\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "GANs\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia (call_2c1q)\n",
      " Call ID: call_2c1q\n",
      "  Args:\n",
      "    query: Neural Networks\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Neural network (machine learning)\n",
      "Summary: In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal bra\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "# user_input=input(\"What do you want to research about?\")\n",
    "user_input=\"GANs\"\n",
    "messages = {\"messages\": HumanMessage(content=user_input)}\n",
    "\n",
    "thread={\"configurable\":{\"thread_id\":\"arxiv_call_1\"}}\n",
    "# messages = scienceBlogCreator.invoke({\"messages\":messages})\n",
    "# messages={'messages':HumanMessage(content=messages)}\n",
    "# scienceBlogCreator.stream(messages,thread,stream_mode=\"values\")\n",
    "for event in scienceBlogCreator.stream(messages,thread,stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Neural network (machine learning)\n",
      "Summary: In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal bra\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got __end__\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscienceBlogCreator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2024\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2018\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2019\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2020\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2021\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2023\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2031\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    228\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\utils\\runnable.py:548\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m             \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    547\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\utils\\runnable.py:302\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m     context = copy_context()\n\u001b[32m    301\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    304\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\write.py:96\u001b[39m, in \u001b[36mChannelWrite._write\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     writes = [\n\u001b[32m     89\u001b[39m         ChannelWriteEntry(write.channel, \u001b[38;5;28minput\u001b[39m, write.skip_none, write.mapper)\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write.value \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writes\n\u001b[32m     95\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\write.py:143\u001b[39m, in \u001b[36mChannelWrite.do_write\u001b[39m\u001b[34m(config, writes, require_at_least_one_of)\u001b[39m\n\u001b[32m    141\u001b[39m     tuples.append((TASKS, w))\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ww := \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    144\u001b[39m         tuples.extend(ww)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\graph\\state.py:721\u001b[39m, in \u001b[36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    717\u001b[39m     msg = create_error_message(\n\u001b[32m    718\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    719\u001b[39m         error_code=ErrorCode.INVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[32m    720\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: Expected dict, got __end__\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "During task with name 'human_feedback' and id '040fd66a-82ba-1ad6-d4c8-bf995fea36de'"
     ]
    }
   ],
   "source": [
    "for event in scienceBlogCreator.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scienceBlogCreator.get_state(thread).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "GANs\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_gswp)\n",
      " Call ID: call_gswp\n",
      "  Args:\n",
      "    query: GANs\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "[Document(metadata={'Published': '2021-11-26', 'Title': 'Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey', 'Authors': 'Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley', 'Summary': 'This is a tutorial and survey paper on Generative Adversarial Network (GAN),\\nadversarial autoencoders, and their variants. We start with explaining\\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\\nand DCGAN. The mode collapse problem is introduced and various methods,\\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\\nintroduce some applications of GAN such as image-to-image translation\\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\\nGAN), text-to-image translation (including StackGAN), and mixing image\\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\\nautoencoders based on adversarial learning including adversarial autoencoder,\\nPixelGAN, and implicit autoencoder.', 'entry_id': 'http://arxiv.org/abs/2111.13282v1', 'published_first_time': '2021-11-26', 'comment': 'To appear as a part of an upcoming textbook on dimensionality\\n  reduction and manifold learning', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML'], 'links': ['http://arxiv.org/abs/2111.13282v1', 'http://arxiv.org/pdf/2111.13282v1']}, page_content='To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.\\nGenerative Adversarial Networks and Adversarial Autoencoders:\\nTutorial and Survey\\nBenyamin Ghojogh\\nBGHOJOGH@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAli Ghodsi\\nALI.GHODSI@UWATERLOO.CA\\nDepartment of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,\\nData Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada\\nFakhri Karray\\nKARRAY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nCentre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada\\nMark Crowley\\nMCROWLEY@UWATERLOO.CA\\nDepartment of Electrical and Computer Engineering,\\nMachine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada\\nAbstract\\nThis is a tutorial and survey paper on Generative\\nAdversarial Network (GAN), adversarial autoen-\\ncoders, and their variants. We start with explain-\\ning adversarial learning and the vanilla GAN.\\nThen, we explain the conditional GAN and\\nDCGAN. The mode collapse problem is intro-\\nduced and various methods, including minibatch\\nGAN, unrolled GAN, BourGAN, mixture GAN,\\nD2GAN, and Wasserstein GAN, are introduced\\nfor resolving this problem. Then, maximum like-\\nlihood estimation in GAN are explained along\\nwith f-GAN, adversarial variational Bayes, and\\nBayesian GAN. Then, we cover feature match-\\ning in GAN, InfoGAN, GRAN, LSGAN, energy-\\nbased GAN, CatGAN, MMD GAN, LapGAN,\\nprogressive GAN, triple GAN, LAG, GMAN,\\nAdaGAN, CoGAN, inverse GAN, BiGAN, ALI,\\nSAGAN, Few-shot GAN, SinGAN, and interpo-\\nlation and evaluation of GAN. Then, we intro-\\nduce some applications of GAN such as image-\\nto-image translation (including PatchGAN, Cy-\\ncleGAN, DeepFaceDrawing, simulated GAN, in-\\nteractive GAN), text-to-image translation (in-\\ncluding StackGAN), and mixing image charac-\\nteristics (including FineGAN and MixNMatch).\\nFinally, we explain the autoencoders based on\\nadversarial learning including adversarial au-\\ntoencoder, PixelGAN, and implicit autoencoder.\\n1. Introduction\\nSuppose we have a generative model which takes a ran-\\ndom noise as input and generates a data point. We want\\nthe generated data point to be of good quality; hence, we\\nshould somehow judge its quality. One way to judge it is\\nto observe the generated sample and assess its quality vi-\\nsually. In this case, the judge is a human. However, we\\ncannot take derivative of human’s judgment for optimiza-\\ntion. Generative Adversarial Network (GAN), proposed in\\n(Goodfellow et al., 2014), has the same idea but it can take\\nderivative of the judgment. For that, it uses a classiﬁer as\\nthe judge rather than a human. Hence, we have a generator\\ngenerating a sample and a binary classiﬁer (or discrimina-\\ntor) to classify the generated sample as a real or generated\\nsample. This classiﬁer can be a pre-trained network which\\nis already trained by some real and generated (fake) data\\npoints. However, GAN puts a step ahead and lets the clas-\\nsiﬁer be trained simultaneously with training the genera-\\ntor. This is the core idea of adversarial learning with the\\nclassiﬁer, also called the discriminator, and the generator\\ncompete each other; hence, they make each other stronger\\ngradually by this competition (Goodfellow et al., 2020).\\nIt is noteworthy that the term “adversarial” is used in two\\nmain streams of research in machine learning and they\\narXiv:2111.13282v1  [cs.LG]  26 Nov 2021\\nGenerative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey\\n2\\nshould not be confused. These two research areas are:\\n• Adversarial attack, also called learning with adversar-\\nial examples or adversarial machine learning. This\\nline of research inspects some examples which can\\nbe changed slightly but wisely to fool a trained learn-\\ning model. For example, perturbation of some speciﬁc\\npixels in the input image may change the decision of\\nlearning model. The reason for this can be analyzed\\ntheoretically. Some example works in this area are\\n(Huang et al., 2011; Moosavi-Dezfooli et al., 2016;\\nKurakin et al., 2017a;b; Madry et al., 2018).\\n• Adversarial learning for generation. This line of re-\\nsearch is categorized as generative models (Ng & Jor-\\ndan, 2002) and/or methods based on that. GAN is in\\nthis line of research. This paper focuses on this re-\\nsearch area.\\nAnother good tutorial on GAN is (Goodfellow, 2016) but it\\ndoes not cover most recent methods in adversarial learning.\\nAlso, an honorary introduction of GAN, by several main\\ncontributors of GAN, is (Goodfellow et al., 2020). Some\\nother existing surveys on GAN are (Wang et al., 2017;\\nCreswell et al., 2018; Gonog & Zhou, 2019; Hong et al.,\\n2019; Pan et al., 2019). This paper is a tutorial and survey\\non GAN and its variants.\\nRequired Background for the Reader\\nThis paper assumes that the reader has general knowledge\\nof calculus, probability, linear algebra, and basics of opti-\\nmization.\\n2. Generative Adversarial Network (GAN)\\n2.1. Adversarial Learning: The Adversarial Game\\nThe original GAN, also called the vanilla GAN, was\\nproposed in (Goodfellow et al., 2014).\\nConsider a d-\\ndimensional dataset with n data points, i.e., {xi\\n∈\\nRd}n\\ni=1. In GAN, we have a generator G which takes a\\np-dimensional random noise z ∈Rp as input and outputs\\na d-dimensional generated point x ∈Rd. Hence, it is the\\nmapping G : z →x where:\\nG(z) = x.\\n(1)\\nThe random noise can be seen as a latent f'), Document(metadata={'Published': '2019-04-01', 'Title': 'GAN You Do the GAN GAN?', 'Authors': 'Joseph Suarez', 'Summary': \"Generative Adversarial Networks (GANs) have become a dominant class of\\ngenerative models. In recent years, GAN variants have yielded especially\\nimpressive results in the synthesis of a variety of forms of data. Examples\\ninclude compelling natural and artistic images, textures, musical sequences,\\nand 3D object files. However, one obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning's most pressing questions: GAN you do\\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\\nGANs? We release the full source code for this project under the MIT license.\", 'entry_id': 'http://arxiv.org/abs/1904.00724v1', 'published_first_time': '2019-04-01', 'comment': '3 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.LG'], 'links': ['http://arxiv.org/abs/1904.00724v1', 'http://arxiv.org/pdf/1904.00724v1']}, page_content='GAN You Do the GAN GAN?\\nJoseph Suarez\\nAbstract\\nGenerative Adversarial Networks (GANs) have\\nbecome a dominant class of generative models.\\nIn recent years, GAN variants have yielded es-\\npecially impressive results in the synthesis of a\\nvariety of forms of data. Examples include com-\\npelling natural and artistic images, textures, mu-\\nsical sequences, and 3D object ﬁles. However,\\none obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning’s most\\npressing questions: GAN you do the GAN GAN?\\nThat is, is it possible to train a GAN to model a\\ndistribution of GANs? We release the full source\\ncode for this project under the MIT license.1\\n1. Introduction, Background, Related Work\\nGANs (Goodfellow et al., 2014) have become perhaps the\\nsingle most prevalent class of generative models in recent\\nyears, spawning hundreds of variants and an entire subﬁeld\\nof surrounding work. While they are notoriously difﬁcult to\\ntrain and often suffer from mode collapse, under the right\\nconditions, GANs have been shown to generate compelling\\nartiﬁcial data distributions and are perhaps best known for\\nrealistic image synthesis (Zhu et al., 2017).\\nOur objective is to apply GANs to a different class of data:\\nGANs themselves. Instead of using a GAN to model images,\\nwe use a GAN to model a distribution of GANs that model\\nimages. We refer to this architecture over architectures\\nas a GAN-GAN. We do not consider GAN-GAN-GANs\\nor GAN-GAN-GAN-GANs in this work. While these are\\nstraightforward to implement, they would require an ex-\\nponential parameter budget (at least as formulated in the\\npresent work) which we lack the hardware to support.\\nHypernetworks (Ha et al., 2016) and extensions thereof\\n(Deutsch, 2018; Suarez, 2017) have also explored the con-\\ncept of using one network to generate the weights of another.\\nHowever, to our knowledge, all such settings typically learn\\nboth the architecture and the meta-architecture end-to-end.\\nIn contrast, we are interested in whether it is possible to\\ndirectly learn a distribution over architectures within the\\n1Full source code:\\nhttps://github.com/jsuarez5341/gan-you-do-the-gan-gan\\nFigure 1. GAN-GAN: a GAN trained on a dataset of GANs\\nstandard setting of generative modeling. We treat GANs\\nthemselves as examples and learn a GAN-GAN by training\\non small set of trained GANs.\\nThis paper is a joke; however, the results are in fact real.\\nInterpretability is a key problem in deep learning, especially\\nin models to be deployed in real world systems. Generative\\nmodeling over networks could serve as a useful tool for\\nvisualization and analysis of network decisions and results.\\n2. Methods\\nGANs formulate a two player game between networks. For-\\nmally, GANs deﬁnes a Generator G and a Discriminator\\nD. The Discriminator models the probability P(G|x) that\\na given example x is fake. The Generator maximizes the\\nprobability P(D(G(z))) (z is sampled noise) that the Dis-\\ncriminator will output an incorrect prediction. A GAN-GAN\\nis simply a GAN trained on a dataset of GAN weights.\\nAlgorithm 1 GAN-GAN Training. We ﬁrst train a set of\\nGANs and save snapshots of the parameters each epoch. We\\nthen train a GAN-GAN (a GAN over GANS) by treating\\neach snapshot as an individual training example.\\nfor GAN Index = 1...#Networks do\\nInitialize an MNIST GAN\\nfor Epoch = 1...#Epochs do\\nTrain the MNIST GAN for one epoch\\nSave a snapshot of the GAN parameters\\nend for\\nend for\\nLoad all snapshots of all GANs into a dataset with #Net-\\nworks × #Epochs examples\\nTrain a GAN over the dataset of GAN snapshots\\narXiv:1904.00724v1  [cs.CV]  1 Apr 2019\\nGAN-GAN\\nFigure 2. Example samples from the training of an MNIST GAN (top-bottom left-right: epochs 1, 2, 10, 25, 27, 30, 32, 35, 40, 49)\\nFigure 3. Image samples from GANs sampled from the trained GAN-GAN. Rows correspond to GANs linearly sampled from 1D\\nGAN-GAN latent space in the interval (-2, 2). Columns correspond to a particular noise vector input to all GANs.\\nGAN-GAN\\n3. Experiments and Discussion\\nGAN Architecture\\nThe generator and discriminator are\\nthree layer (input-hidden-output) fully connected neural net-\\nworks with hidden dimension 64. We use Leaky ReLU(Xu\\net al., 2015) activations with 0.2 negative slope after the\\ninput and hidden layers. We use tanh for the generator out-\\nput (dimensionality 28 × 28 = 784 to match MNIST) and\\nsigmoid for the discriminator output (dimensionality 1). The\\ngenerator samples from latent dimension 64.\\nGAN-GAN Architecture\\nThe GAN-GAN generator and\\ndiscriminator have the same layer and activation structure\\nas the MNIST GAN. The input dimensionality is 113745,\\nwhich is equal to the dimensionality of the GAN parameter\\nvector. We found that using a smaller hidden dimension\\nfor the discriminator (8) than the generator (64) helped to\\nstabilize training. We use latent dimension 1 for the GAN-\\nGAN in order to enable visualizations. Results improve\\nwith a larger latent space.\\nTraining\\nWe use Adam(Kingma & Ba, 2014) for all net-\\nworks. The learning rate is ﬁxed to 0.0002; all other pa-\\nrameters are PyTorch defaults. We use batch sizes 128 and\\n32 for MNIST and the GAN-GAN, respectively. As de-\\nscribed in Algorithm 1, we train 35 MNIST GANs for 100\\nepochs each, saving snapshots of the weights at each epoch.\\nWe train the GAN-GAN for 250 epochs using these 3500\\nsnapshots as training examples.\\nResults\\nIn order to evaluate the performance of the GAN-\\nGAN, we ﬁrst linearly sample 32 GANs from the 1-\\ndimensional latent space of the GAN-GAN. We then ﬁx\\n40 noise'), Document(metadata={'Published': '2023-03-27', 'Title': 'Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances', 'Authors': 'Arkanath Pathak, Nicholas Dufour', 'Summary': 'Modern Generative Adversarial Networks (GANs) generate realistic images\\nremarkably well. Previous work has demonstrated the feasibility of\\n\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\\noperate on images generated from a frozen GAN. That such classifiers work at\\nall affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts\\nacross samples) present in GAN training. We iteratively train GAN-classifiers\\nand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge\\ngaps), and examine the effect on GAN training dynamics, output quality, and\\nGAN-classifier generalization. We investigate two settings, a small DCGAN\\narchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA\\nGAN architecture trained on high dimensional images (FFHQ). We find that the\\nDCGAN is unable to effectively fool a held-out GAN-classifier without\\ncompromising the output quality. However, StyleGAN2 can fool held-out\\nclassifiers with no change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to reveal an ordering\\nover optima in the generator parameter space. Finally, we study different\\nclassifier architectures and show that the architecture of the GAN-classifier\\nhas a strong influence on the set of its learned artifacts.', 'entry_id': 'http://arxiv.org/abs/2303.15533v1', 'published_first_time': '2023-03-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CV'], 'links': ['http://arxiv.org/abs/2303.15533v1', 'http://arxiv.org/pdf/2303.15533v1']}, page_content='Sequential training of GANs against GAN-classifiers reveals correlated\\n“knowledge gaps” present among independently trained GAN instances\\nArkanath Pathak\\nNicholas Dufour\\nGoogle Research\\n{arkanath,ndufour}@google.com\\nAbstract\\nModern Generative Adversarial Networks (GANs) gen-\\nerate realistic images remarkably well. Previous work has\\ndemonstrated the feasibility of “GAN-classifiers” that are\\ndistinct from the co-trained discriminator, and operate on\\nimages generated from a frozen GAN. That such classifiers\\nwork at all affirms the existence of “knowledge gaps” (out-of-\\ndistribution artifacts across samples) present in GAN train-\\ning. We iteratively train GAN-classifiers and train GANs that\\n“fool” the classifiers (in an attempt to fill the knowledge gaps),\\nand examine the effect on GAN training dynamics, output\\nquality, and GAN-classifier generalization. We investigate\\ntwo settings, a small DCGAN architecture trained on low\\ndimensional images (MNIST), and StyleGAN2, a SOTA GAN\\narchitecture trained on high dimensional images (FFHQ).\\nWe find that the DCGAN is unable to effectively fool a held-\\nout GAN-classifier without compromising the output quality.\\nHowever, the StyleGAN2 can fool held-out classifiers with\\nno change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to\\nreveal an ordering over optima in the generator parameter\\nspace. Finally, we study different classifier architectures and\\nshow that the architecture of the GAN-classifier has a strong\\ninfluence on the set of its learned artifacts.\\n1. Introduction\\nGAN [9] architectures like StyleGAN2 [18] generate\\nhigh-resolution images that appear largely indistinguishable\\nfrom real images to the untrained eye [15, 19, 25]. While\\nthere are many positive applications, the ability to generate\\nlarge amounts of realistic images is also a source of concern\\ngiven its potential application in scaled abuse and misin-\\nformation. In particular, GAN-generated human faces are\\nwidely available (e.g., thispersondoesnotexist.com) and have\\nbeen used for creating fake identities on the internet [13].\\nDetection of GAN-generated images is an active research\\narea (see [10] for a survey of approaches), with some us-\\ning custom methods and others using generic CNN-based\\nclassifiers. Such classifiers are distinct from the discrimina-\\ntor networks that are trained alongside the generator in the\\narchetypal GAN setup. Given the adversarial nature of the\\ntraining loss for GANs, the existence of the GAN-classifiers\\nsuggest consistent generator knowledge gaps (i.e., artifacts\\npresent across samples that distinguish generated images\\nfrom those of the underlying distribution) left by discrimi-\\nnators during training. Specialized classifiers [32] are able\\nto detect images sampled from held-out GAN instances and\\neven from held-out GAN architectures. These generalization\\ncapabilities imply that the knowledge gaps are consistent\\nnot only across samples from a GAN generator but across\\nindependent GAN generator instances.\\nIn this work we modify the GAN training loss in order\\nto fool a GAN-classifier in addition to the co-trained dis-\\ncriminator, and examine the effect on training dynamics and\\noutput quality. We conduct multiple rounds of training in-\\ndependent pools (initialized differently) of GANs followed\\nby GAN-classifiers, and gain new insights into the GAN\\noptimization process. We investigate two different settings:\\nin the first setting, we choose the low-dimensional domain\\nof handwritten digits (MNIST [20]), using a small DCGAN\\n[26] architecture and a vanilla GAN-classifier architecture.\\nFor the second setting, we choose a high-dimensional do-\\nmain of human faces (FFHQ [17]) with StyleGAN2 (SG2)\\nas a SOTA GAN architecture, and three different GAN-\\nclassifier architectures (ResNet-50 [11], Inception-v3 [29],\\nand MobileNetV2 [28]). Our findings in this paper are as\\nfollows:\\n• Samples drawn from a GAN instance exhibit a space of\\n“artifacts” that are exploited by the classifiers, and this\\nspace is strongly correlated with those of other GAN\\ngenerator instances. This effect is present in both the\\nDCGAN and SG2 settings.\\n• Upon introducing the need to fool held-out classifiers,\\nthe DCGAN is unable to generate high quality outputs.\\n• In the high dimensional setting, however, SG2 gener-\\nators can easily fool held-out trained classifiers, and\\nmove to a new artifact space. Strikingly, we find that\\nthe artifact space is correlated among the new popula-\\ntion of generators as it was in the original population.\\nThis correlation appears to persist in subsequent rounds\\nas new classifiers are introduced that are adapted to the\\nnew artifact spaces.\\n• MobileNetV2 classifier instances in the SG2 setting\\nappear unable to learn all of the artifacts available for\\nthem to exploit. Instead, MobileNetV2 instances form\\nclusters based on the subset of artifacts learned. We\\nhypothesize this being an effect of classifier capacity.\\n• An SG2 generator trained to reliably fool unseen classi-\\nfier instances from a given architecture is not guaran-\\nteed to fool classifiers from another architecture. There-\\nfore, the artifacts learned by a given classifier depends\\nstrongly on the classifier’s architecture.\\n2. Related Work\\nResearch into detection of GAN-generated media has\\nlargely tracked the increasing prominence and output quality\\nof GANs themselves. Several studies [4, 5, 6, 10, 14, 22,\\n24, 31, 32] focus on detection of GAN-generated images\\nusing CNNs, and thei')]\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Based on the next user answer, decide if you should return to the tools \n",
      "                      node to make a wikipedia search or if you should go to END.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "yes, research adversarial networks\n"
     ]
    }
   ],
   "source": [
    "new_state = scienceBlogCreator.get_state(thread).values\n",
    "for m in new_state['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "yes, research adversarial networks\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got tools\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Continue the graph execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscienceBlogCreator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2024\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2018\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2019\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2020\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2021\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2023\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2031\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    228\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\utils\\runnable.py:548\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m             \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    547\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\utils\\runnable.py:302\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m     context = copy_context()\n\u001b[32m    301\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    304\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\write.py:96\u001b[39m, in \u001b[36mChannelWrite._write\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     writes = [\n\u001b[32m     89\u001b[39m         ChannelWriteEntry(write.channel, \u001b[38;5;28minput\u001b[39m, write.skip_none, write.mapper)\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write.value \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writes\n\u001b[32m     95\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\pregel\\write.py:143\u001b[39m, in \u001b[36mChannelWrite.do_write\u001b[39m\u001b[34m(config, writes, require_at_least_one_of)\u001b[39m\n\u001b[32m    141\u001b[39m     tuples.append((TASKS, w))\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ww := \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    144\u001b[39m         tuples.extend(ww)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Valentin\\Documents\\VJDS\\Programming\\youtube\\DaveEbbelaar\\Agents\\Blog-CodeReviewer-Orchestrator\\envBlog\\Lib\\site-packages\\langgraph\\graph\\state.py:721\u001b[39m, in \u001b[36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    717\u001b[39m     msg = create_error_message(\n\u001b[32m    718\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    719\u001b[39m         error_code=ErrorCode.INVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[32m    720\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: Expected dict, got tools\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "During task with name 'human_feedback' and id 'b1a7fc18-4fa8-8102-e3d4-4cea58db575b'"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution\n",
    "for event in scienceBlogCreator.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ tools_condition decision: __end__\n"
     ]
    }
   ],
   "source": [
    "state_after_feedback = scienceBlogCreator.get_state(thread).values\n",
    "decision = tools_condition(state_after_feedback)\n",
    "print(f\"🛠️ tools_condition decision: {decision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
